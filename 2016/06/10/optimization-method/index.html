<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="最优化,梯度下降,牛顿法," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="如果从事过机器学习相关工作都应该对梯度下降和牛顿法等不陌生。统计学习的角度来说(其实是李航老师说的)，大部分机器学习主要分为三个块：模型，策略和算法。其中算法的目标就是求解最优参数的过程，包含了各种最优化方法。而梯度下降就是其中最普遍的一个，例如：线性回归，神经网络、逻辑回归等算法都经常用梯度下降法完成最优化过程。说到梯度下降，我和很多人一样，曾经的第一反应就是：“它是一个不断迭代的算法，每一次在">
<meta name="keywords" content="最优化,梯度下降,牛顿法">
<meta property="og:type" content="article">
<meta property="og:title" content="常用最优化方法总结">
<meta property="og:url" content="http://note.hustkerry.me/2016/06/10/optimization-method/index.html">
<meta property="og:site_name" content="Kerry">
<meta property="og:description" content="如果从事过机器学习相关工作都应该对梯度下降和牛顿法等不陌生。统计学习的角度来说(其实是李航老师说的)，大部分机器学习主要分为三个块：模型，策略和算法。其中算法的目标就是求解最优参数的过程，包含了各种最优化方法。而梯度下降就是其中最普遍的一个，例如：线性回归，神经网络、逻辑回归等算法都经常用梯度下降法完成最优化过程。说到梯度下降，我和很多人一样，曾经的第一反应就是：“它是一个不断迭代的算法，每一次在">
<meta property="og:image" content="http://note.hustkerry.me/img/optimization_1.png">
<meta property="og:image" content="http://note.hustkerry.me/img/optimization_2.png">
<meta property="og:image" content="http://note.hustkerry.me/img/optimization_3.png">
<meta property="og:image" content="http://note.hustkerry.me/img/optimization_4.png">
<meta property="og:updated_time" content="2017-05-19T08:21:47.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="常用最优化方法总结">
<meta name="twitter:description" content="如果从事过机器学习相关工作都应该对梯度下降和牛顿法等不陌生。统计学习的角度来说(其实是李航老师说的)，大部分机器学习主要分为三个块：模型，策略和算法。其中算法的目标就是求解最优参数的过程，包含了各种最优化方法。而梯度下降就是其中最普遍的一个，例如：线性回归，神经网络、逻辑回归等算法都经常用梯度下降法完成最优化过程。说到梯度下降，我和很多人一样，曾经的第一反应就是：“它是一个不断迭代的算法，每一次在">
<meta name="twitter:image" content="http://note.hustkerry.me/img/optimization_1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://note.hustkerry.me/2016/06/10/optimization-method/"/>





  <title>常用最优化方法总结 | Kerry</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Kerry</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Kerry的个人笔记</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://note.hustkerry.me/2016/06/10/optimization-method/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kerry">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/img/python.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kerry">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">常用最优化方法总结</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2016-06-10T10:38:08+08:00">
                2016-06-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>如果从事过机器学习相关工作都应该对梯度下降和牛顿法等不陌生。统计学习的角度来说(其实是李航老师说的)，大部分机器学习主要分为三个块：模型，策略和算法。其中算法的目标就是求解最优参数的过程，包含了各种最优化方法。而梯度下降就是其中最普遍的一个，例如：线性回归，神经网络、逻辑回归等算法都经常用梯度下降法完成最优化过程。说到梯度下降，我和很多人一样，曾经的第一反应就是：“<strong>它是一个不断迭代的算法，每一次在点下降最快的方向(就是梯度反方向)往前走一个步长(这个步长可以定长，也可以动态调整，步长偏大容易震荡，偏小又导致训练速度下降)，如此不停迭代，直到损失函数不再有明显变化。当然，为了缩短训练时间或者防止过拟合也可以early stop，例如：如果validation data的准确率不再发生明显变化了，就停止训练。</strong>”了解这些也许对于工程实现来说够了，但是仔细一想，什么是梯度，为什么沿着梯度反方向下降是最快的，它的原理是什么？所以，我觉得需要总结一下最优化方法。<a id="more"></a></p>
<p>一般优化方法分为：梯度下降（批量、随机、mini-batch）；牛顿法（包含为了不计算hesse矩阵而提出的拟牛顿法）；共轭梯度法；启发式方法（模拟退火、遗传算法、蚁群算法等）。这里对梯度下降和牛顿法做一个总结，至于其他优化算法，日后若有契机的话再做总结。</p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p><strong>什么是梯度？</strong>梯度是一个方向，对于某个优化函数，只要点确定了，这个方向就是定的，沿着这个方向优化函数上升最快，逆这个方向优化函数下降最快。梯度下降法是求解无约束最优化问题的常用方法。来个正规一点的定义：<strong>欧几里得空间R<sup>n</sup>里的标量值函数f(x<sub>1</sub>,x<sub>2</sub>,…,x<sub>n</sub>)关于每一个变量x<sub>i</sub>都具有一阶偏导数。那么在点a处，这些一阶偏导数就定义了一个向量：<br><img src="/img/optimization_1.png" alt="optimization_1"><br>这个向量就称为f在点a处的梯度。</strong>如果目标函数是凸函数的时候，梯度下降的解是全局最优解，但一般情况下这是很难保证的。同时，梯度下降算法的收敛速度也不一定是最快的，例如牛顿法从理论上讲就比梯度下降快。</p>
<p><strong>梯度方向是怎么找出来的？为什么沿着这个方向目标函数上升最快？</strong>回答这个问题，需要这个问题需要借助泰勒展开。假设f(x)具有一阶连续偏导数，若第k次迭代值为x<sup>(k)</sup>，那么可以将f(x)在x<sup>(k)</sup>处做一阶泰勒展开：<strong>f(x) = f(x<sup>(k)</sup>) + ▽f(x<sup>(k)</sup>)(x - x<sup>(k)</sup>)</strong> 代入x<sup>(k+1)</sup>可以得到f(x<sup>(k+1)</sup>) = f(x<sup>(k)</sup>) + ▽f(x<sup>(k)</sup>)(x<sup>(k+1)</sup> - x<sup>(k)</sup>)，我们知道梯度下降是一种迭代算法，那么假设x<sup>(k+1)</sup> = x<sup>(k)</sup> + λ<sub>k</sub>p<sub>k</sub>，其中λ<sub>k</sub>就是我们说的步长，p<sub>k</sub>是移动的方向。步长我们这里不讨论，关键是方向，如果找到一个最合适的方向使得下降最快。也就是说找一个p<sub>k</sub>使得f(x<sup>(k+1)</sup>)最小，如下：<br><img src="/img/optimization_2.png" alt="optimization_2"><br>因为步长λ<sub>k</sub> &gt; 0, 所以很明显当p<sub>k</sub> = -▽f(x<sup>(k)</sup>)的时候，下降最快，那么就得到了梯度下降法的迭代公式：<strong>x<sup>(k+1)</sup> = x<sup>(k)</sup> - λ<sub>k</sub>▽f(x<sup>(k)</sup>)</strong>。熟悉泰勒展开的应该很容易理解上面的过程，一天在知乎上看到一篇文章也解释了，它是从导数到偏导再到方向导数一步步解释得到，通俗易懂: <a href="https://zhuanlan.zhihu.com/p/24913912" target="_blank" rel="external">👇</a></p>
<p>说完了方向，我们可以稍微回到步长，那么每一步迭代怎么确定一个最优步长呢？其实进一步考虑，其实在每一点，梯度下降法的最优步长也是可以计算的。如果可以的话，我们可以将目标函数进一步做二阶泰勒展开，并将x<sup>(k+1)</sup> = x<sup>(k)</sup> - λ<sub>k</sub>▽f(x<sup>(k)</sup>)代入到展开结果中，然后求关于λ<sub>k</sub>的偏导数=0的解即可，不过这个“最优步长”需要计算hesse矩阵(下面的牛顿法中会提到)。</p>
<h2 id="梯度下降续"><a href="#梯度下降续" class="headerlink" title="梯度下降续"></a>梯度下降续</h2><p>其实，在机器学习算法中，常用的梯度下降是随机梯度下降和mini-batch梯度下降。这两种梯度下降使得梯度下降算法能够online的进行，同时训练速度更快，另外，理论和实践表明，效果不差于批量梯度下降。例如，在神经网络的优化中最常用的就是mini-batch梯度下降。梯度下降中步长，可以是定长，可以变长(不同的变长策略)，而且不同的参数对应的步长可以不一样(尤其是高维稀疏特征)。同时，梯度下降也有很多优化的变种，例如带动量信息的梯度下降和自适应学习率的梯度下降。关于随机梯度下降和各种变种强烈建议去看这篇博文：<a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="external">梯度下降最优化方法概叙</a>，这篇博文讲得很详细，最后对于优化方法的选择也做了阐述，例如一般情况下Adam就是一个不错的选择。</p>
<h2 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h2><p>从上面可以知道，梯度下降的思想是在某一个点用一个平面去拟合函数在该点的局部曲面，那么牛顿法则是用一个二次曲面去拟合函数在该点的局部曲面。梯度下降是通过一阶泰勒展开得到的迭代公式最小化而找出下降最快的方向。牛顿迭代法则通过二阶泰勒展开，然后利用函数在最小值处的梯度为0这个性质推到得到。在说牛顿法求函数最优值之前，先来做一个题目。怎么用计算机求解x<sup>2</sup>-n=0？</p>
<p>求解：令f(x) = x<sup>2</sup> - n, 那么f(x)在点(x<sub>i</sub>, f(x<sub>i</sub>))的切线方程为：f(x) = f(x<sub>i</sub>) + f(<sup>‘</sup>x<sub>i</sub>)(x - x<sub>i</sub>),其中f(<sup>‘</sup>x<sub>i</sub>) = 2x，因为要求f(x) = 0的点，<strong>令f(x<sub>i+1</sub>) = 0，得到：x<sub>i+1</sub> =（x<sub>i</sub> + n/x<sub>i</sub>）/ 2</strong>，如下图所示：<br><img src="/img/optimization_3.png" alt="optimization_3"><br>对应的python代码如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">def sqrt(n):</div><div class="line">  # 初始值</div><div class="line">  x_i_1 = 1.0</div><div class="line">  x_i = -1.0</div><div class="line">  # x不再变化时停止</div><div class="line">  while (x_i_1 != x_i):</div><div class="line">    x_i = x_i_1</div><div class="line">    x_i_1 = (x_i_1 + n/x_i_1)/2.0</div><div class="line">  return x_i_1</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>进一步，牛顿法求解优化问题的思路和上面求解f(x) = 0一致，不同的是优化问题中求解的是最大(小)值，需要转换一下条件，考虑到极值点的梯度为0，所以牛顿法就是通过求解梯度为0这个条件爱解决优化问题的。与梯度下降类似，牛顿法的证明也需要泰勒展开，只不过这次是二阶泰勒展开。</p>
<p>假设f(x)是n维欧几里得空间的标量函数，且具有连续二阶偏导数，第k次迭代值为x<sup>(k)</sup>，那么可以将f(x)在x<sup>(k)</sup>处做一阶泰勒展开：<strong>f(x) = f(x<sup>(k)</sup>) + ▽f(x<sup>(k)</sup>)(x - x<sup>(k)</sup>) + 1/2(x - x<sup>(k)</sup>)<sup>T</sup>H(x<sup>(k)</sup>)(x - x<sup>(k)</sup>)</strong> 其中H(x<sup>(k)</sup>)是f(x)在点x<sup>(k)</sup>处的hesse矩阵：<br><img src="/img/optimization_4.png" alt="optimization_4"> 因为f(x)有极值的必要条件是在极值点处梯度向量为0，所以如果x<sup>(k+1)</sup>是极值点的话，那么▽f(x<sup>(k+1)</sup>) = 0，对二阶泰勒展开求导得到：<strong>▽f(x) = ▽f(x<sup>(k)</sup>) + H(x<sup>(k)</sup>)(x - x<sup>(k)</sup>)</strong>，令▽f(x<sup></sup>) = 0并代入x<sup>(k+1)</sup>就得到了最终迭代公式：<strong>x<sup>(k+1)</sup> = x<sup>(k)</sup> - H(x<sup>(k)</sup>)<sup>-1</sup>▽f(x<sup>(k)</sup>)</strong></p>
<p>对比梯度下降法和牛顿法，梯度下降使用的是梯度寻找下降最快的方向，而牛顿法则通过曲率去寻找下降最快的方向，因此牛顿法的路径更直接，下降更快。但是，牛顿法每次迭代并不能保证每次目标函数值都是下降的，必须要求hesse矩阵是正定矩阵才会下降。<strong>注意：n阶实对称矩阵Q，对于任意的非0向量X，如果有X<sup>T</sup>QX&gt;0，则称Q是正定矩阵。通过迭代公式可知，牛顿迭代法的搜索方向是-H<sub>k</sub><sup>-1</sup>g<sub>k</sub>，改方向必须和梯度方向相反也就是-g<sub>k</sub><sup>T</sup>H<sub>k</sub><sup>-1</sup>g<sub>k</sub><0，函数才下降，也就是说在该点hesse矩阵是正定的，该次迭代的函数值是往下走的。这种情况不一定都成立的，尤其是在距离最优点较远的地方。这里的h<sub>k就是H(x<sup>(k)</sup>)，g<sub>k</sub>就是▽f(x<sup>(k)</sup>)。</0，函数才下降，也就是说在该点hesse矩阵是正定的，该次迭代的函数值是往下走的。这种情况不一定都成立的，尤其是在距离最优点较远的地方。这里的h<sub></strong></p>
<h2 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h2><p>从牛顿法的推导过程中，可以发现，每次迭代都要计算该点的hesse矩阵的逆。这个计算量是很大的。拟牛顿法的迭代中，考虑使用一个n阶矩阵G<sub>k</sub>近似代替hesse矩阵的逆。这就是逆牛顿法的思路。是不是用Hesse矩阵的近似矩阵来代替Hesse矩阵，会导致求解效果变差呢？事实上，效果反而通常会变好。拟牛顿法与牛顿法的迭代过程一样，仅仅是各个Hesse矩阵的求解方法不一样。在远离极小值点处，Hesse矩阵一般不能保证正定，使得目标函数值不降反升。而拟牛顿法可以使目标函数值沿下降方向走下去，并且到了最后，在极小值点附近，可使构造出来的矩阵与Hesse矩阵“很像”了，这样，拟牛顿法也会具有牛顿法的二阶收敛性。常用的有DFP和LBGS两种方法。</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/最优化/" rel="tag"># 最优化</a>
          
            <a href="/tags/梯度下降/" rel="tag"># 梯度下降</a>
          
            <a href="/tags/牛顿法/" rel="tag"># 牛顿法</a>
          
        </div>
      

      
        
      

      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/img/python.png"
               alt="Kerry" />
          <p class="site-author-name" itemprop="name">Kerry</p>
           
              <p class="site-description motion-element" itemprop="description">认真你就输了<br/>一直认真你就赢了</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度下降"><span class="nav-number">1.</span> <span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度下降续"><span class="nav-number">2.</span> <span class="nav-text">梯度下降续</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#牛顿法"><span class="nav-number">3.</span> <span class="nav-text">牛顿法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#拟牛顿法"><span class="nav-number">4.</span> <span class="nav-text">拟牛顿法</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kerry</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  





  

  

  

  

  

</body>
</html>

